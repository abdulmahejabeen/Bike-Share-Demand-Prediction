{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShnLWythrG75"
   },
   "source": [
    "#### ***Importing Necessary Libraries*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vp8QWXQT54bv",
    "outputId": "bda9a115-089f-47fc-941c-001acb2d81ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275 kB 6.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.21.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.7.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (1.0.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.18.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2021.11.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.9.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.6.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->lime) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->lime) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.2.0)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=52f39fef4c8d33d06f8a31827ff96487426aa742f79ea419c93d1cb4c4e8aa1b\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
      "Successfully built lime\n",
      "Installing collected packages: lime\n",
      "Successfully installed lime-0.2.0.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting shap\n",
      "  Downloading shap-0.41.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (569 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 569 kB 7.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.3)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.56.4)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (1.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.7.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.21.6)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.64.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.5)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>20.9->shap) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->shap) (4.13.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.39.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (3.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.2.0)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.41.0 slicer-0.0.7\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "#For Model interpretation\n",
    "!pip install lime\n",
    "!pip install shap\n",
    "\n",
    "#install xgboost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XT85Wvh-rS5D"
   },
   "outputs": [],
   "source": [
    "import numpy as np # mathematical computation\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns # visualization\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') #ignore warnings\n",
    "from io import BytesIO\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "#set style\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Setting font weight,label weight,title weight to bold and setting title size,label size,fontsize.\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.titlesize\"] = 15\n",
    "plt.rcParams[\"axes.titleweight\"] = 'bold'\n",
    "plt.rcParams['xtick.labelsize']=10\n",
    "plt.rcParams['ytick.labelsize']=10\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "plt.rcParams[\"legend.fontsize\"] = 10\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 15\n",
    "\n",
    "#imports for data preparationa and modeling\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error,r2_score,accuracy_score,mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression,ElasticNet,Lasso,Ridge\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#import for creating tree\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "#import pickle Package\n",
    "import pickle\n",
    "\n",
    "#imports for model interpretation\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLtF7BaDrLHv"
   },
   "source": [
    "#### ***Importing Bike share rides data*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2CixkNvrIlt"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rk6IPefXr4rA"
   },
   "source": [
    "#### ***Reading CSV file into dataframe*** :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klPMHTo4r7a2"
   },
   "outputs": [],
   "source": [
    "Bikes_df=pd.read_csv(BytesIO(uploaded['SeoulBikeData.csv']),encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1nvAQBbrILo"
   },
   "source": [
    "#### ***Data Exploration And Data Cleaning*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLvz1sswsTFq"
   },
   "outputs": [],
   "source": [
    "# Check sample 5 rows\n",
    "Bikes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZx2EN_UufxZ"
   },
   "outputs": [],
   "source": [
    "# overview of dimensions  and datatypes of the dataset \n",
    "print(f'The shape of dataset is {(Bikes_df.shape)} \\n Total Rows are : {(Bikes_df.shape)[0]}\\n Total Columns are : {(Bikes_df.shape)[1]}')\n",
    "print('\\n')\n",
    "Bikes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kC61JKqsIGVe"
   },
   "outputs": [],
   "source": [
    "#Dealing with invalid data types\n",
    "Bikes_df['Date']=Bikes_df['Date'].astype('datetime64[ns]')\n",
    "Bikes_df['Seasons']=Bikes_df['Seasons'].astype('category')\n",
    "Bikes_df['Holiday']=Bikes_df['Holiday'].astype('category')\n",
    "Bikes_df['Hour']=Bikes_df['Hour'].astype('category')\n",
    "Bikes_df['Functioning Day']=Bikes_df['Functioning Day'].astype('category')\n",
    "# Renaming columns for better analysis\n",
    "Bikes_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count','Temperature(Â°C)':'Temperature','Humidity(%)':'Humidity','Wind speed (m/s)':'Wind_speed',\n",
    "                       'Visibility (10m)':'Visibility','Dew point temperature(Â°C)':'Dew_point_temperature', 'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
    "                        'Rainfall(mm)':'Rainfall','Snowfall (cm)':'Snowfall','Functioning Day':'Functioning_Day'},inplace=True)\n",
    "\n",
    "#Extracting Month and Day of the week from Date\n",
    "Bikes_df['Month']=Bikes_df['Date'].dt.month_name().astype('category')\n",
    "Bikes_df['Day']=Bikes_df['Date'].dt.day_name().astype('category')\n",
    "Bikes_df[\"year\"] = Bikes_df['Date'].map(lambda x: x.year).astype(\"category\")\n",
    "\n",
    "#Adding Weekend column from day as 1 for weekedn and 0 for weekday\n",
    "Bikes_df['Weekend']=Bikes_df['Day'].apply(lambda x: 1 if x=='Saturday' or x=='Sunday' else 0)\n",
    "Bikes_df['Weekend']=Bikes_df['Weekend'].astype('category')\n",
    "# drop the Date column\n",
    "Bikes_df.drop(columns=['Date'],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5Id5sihJtnx"
   },
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "print(f'Null values :\\n{Bikes_df.isnull().sum()}')\n",
    "print('\\n')\n",
    "#Checking for duplicate values\n",
    "print(f'Duplicate values : {sum(Bikes_df.duplicated())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6vTjq9SSfot"
   },
   "outputs": [],
   "source": [
    "Bikes_df.describe().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQpVsQmbKvYg"
   },
   "source": [
    "#### ***Exploratory Data Analysis :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9R0c8k8npi7N"
   },
   "outputs": [],
   "source": [
    "Bikes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brQYFDxgLY3i"
   },
   "source": [
    "***When are the most rides happening ?*** ðŸ“…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCg0ktknbp7N"
   },
   "outputs": [],
   "source": [
    "numeric_features= Bikes_df.select_dtypes(exclude=['category','datetime64'])\n",
    "categorical_features=Bikes_df.select_dtypes(include='category')\n",
    "# Plot barchart plot for each categorical columns with respect to target variable to get insights\n",
    "for col in categorical_features:\n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    ax = fig.gca()\n",
    "    sns.barplot(x=Bikes_df[col],y=Bikes_df['Rented_Bike_Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zmj6ABl7Zw3"
   },
   "source": [
    "\n",
    "\n",
    ">***Observations:***\n",
    "\n",
    "*  ***Time of day:*** It is evident that the peaks are observed in mornings at 8AM & 9AM and evenings at 4PM, 5PM & 6PM implicating that majority of the trips are taken before and after the usual office hours. Moreover, the trips taken in between the usual office hours i.e. 10AM â€” 3PM are constant. Additionaly, the trips have declined substantially after 6PM.\n",
    "\n",
    "*   ***Day of week:*** As majority of the trips were taken before and after the usual office hours, it is no surprise that majority of the trips have taken place at weekdays (Mon-Fri) as compared to weekends (Sat-Sun)\n",
    "\n",
    "*   ***Season :*** We observe that summer has the highest rentals followed by spring and then fall, which gives us reason to believe that the bike riders prefer warm to pleasant climates than colder climates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4RNYkMZoLZQ"
   },
   "source": [
    "##### ***What is the distribution of rides on different kinds of days wrt Hours?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED5SRA4noWRK"
   },
   "outputs": [],
   "source": [
    "for i in categorical_features:\n",
    "  if i == 'Hour':\n",
    "    pass\n",
    "  else:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.pointplot(x=Bikes_df[\"Hour\"],y=Bikes_df['Rented_Bike_Count'],hue=Bikes_df[i])\n",
    "    plt.title(f\"Rented Bike Count during different {i} with respect of Hour\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ER4bh7Vr-FDm"
   },
   "source": [
    "\n",
    "\n",
    "> ***Observations :***\n",
    "\n",
    "\n",
    "* We can observe that the pattern of weekdays and weekends as well as Holidays is different, in the weekend and on holiday the demand becomes high in the afternoon. While the demand for office timings is high during weekdays and no holidays.\n",
    "\n",
    "* In the month column, We can clearly see that the demand is low in December January & Febuary,  as it is cold in these months and we have already seen in season column that demand is less in winters.\n",
    "\n",
    "*  Comparitively the demand was high in 2018 than in 2017, which can support the fact that bike share demand is increasing over the years.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcxYvo_3qmGs"
   },
   "source": [
    "#### ***What is the distribution and correlation among variables ?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyeKlaZbta9c"
   },
   "outputs": [],
   "source": [
    "#plotting histogram with mean and median, and distplot of all the numeric features of the dataset\n",
    "n=1\n",
    "for i in numeric_features.columns:\n",
    "  plt.figure(figsize=(20,40))\n",
    "  plt.subplot(9,2,n)\n",
    "  n+=1\n",
    "  print('\\n')\n",
    "  print('*'*70,i,'*'*70)\n",
    "  print('\\n')\n",
    "  # fig=plt.figure()\n",
    "  # ax=fig.gca()\n",
    "  feature=Bikes_df[i]\n",
    "  feature.hist(bins=50,)\n",
    "  plt.axvline(feature.mean(), color='purple', linestyle='dashed', linewidth=2)\n",
    "  plt.axvline(feature.median(), color='coral', linestyle='dashed', linewidth=2) \n",
    "  plt.subplot(9,2,n)\n",
    "  n+= 1\n",
    "  sns.distplot(Bikes_df[i])\n",
    "  # plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj1oNdk_tDCW"
   },
   "outputs": [],
   "source": [
    "# Regression plot to know relation of target variable with our independent variable\n",
    "n=1\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in numeric_features.columns:\n",
    "  if i == 'Rented_Bike_Count':\n",
    "    pass\n",
    "  else:\n",
    "    plt.subplot(4,2,n)\n",
    "    n+=1\n",
    "    sns.regplot(Bikes_df[i], Bikes_df['Rented_Bike_Count'],scatter_kws={\"color\": \"pink\"}, line_kws={\"color\": \"green\"})\n",
    "    plt.title(f'Target variable and {i}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7fXmqjFFyOu"
   },
   "outputs": [],
   "source": [
    "#correlation of all the numerical features with the heat map, so that we will also get to know the multicolinearity\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(Bikes_df.select_dtypes(include=['float','int']).corr(),annot=True,center = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z82swanyv1tR"
   },
   "outputs": [],
   "source": [
    "#Checking for Data imbalance by creating pieplot\n",
    "n=1\n",
    "plt.figure(figsize=(20,15))\n",
    "columns=['Hour','Seasons','Month']\n",
    "for i in columns:\n",
    "  plt.subplot(3,3,n)\n",
    "  n=n+1\n",
    "  plt.pie(Bikes_df[i].value_counts(),labels = Bikes_df[i].value_counts().keys().tolist(),autopct='%.0f%%')\n",
    "  plt.title(i)\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3CYguLjCFSj"
   },
   "source": [
    "\n",
    "\n",
    ">***Observations :***\n",
    "\n",
    "\n",
    "*   From the distplots we could see that some of the variables are either right or lwft skewed and for those variables, even the mean and median are skewed as seen in the histograms.\n",
    "\n",
    "*  Regression plots show that the few variables are positively correlated and few are nagtively correlated with our target variable.\n",
    "\n",
    "* From Heat map, we could see that there is multicollineriaty between few variables like Temperature and Dew point temperature.\n",
    "\n",
    "* From the pie plots , we could see that the data is uniformly distributed across all seasons, months and hours. Hence, there is no data imbalance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El8jhvFJOlDk"
   },
   "source": [
    "#### ***Preparing Data for Modeling*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lMh2KUZOoPE"
   },
   "outputs": [],
   "source": [
    "# Make copy of dataset\n",
    "df=Bikes_df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8Ugdxk_m3uW"
   },
   "source": [
    "***Multicollinearity*** can be a problem in a regression model because we would not be able to distinguish between the individual effects of the independent variables on the dependent variable.It can be detected via various methods. \n",
    "In this project, we used the most common approach ***VIF (Variable Inflation Factors).***\n",
    "\n",
    "VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.If VIF >5, then variables are highly collinear and the parameter estimates will have large stanhdard errors due to this.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nHppEEcQrbt"
   },
   "outputs": [],
   "source": [
    "#Detecting Multicollinearity using VIF(Variable Inflation Factors)\n",
    "def Calculate_vif(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return(vif)\n",
    "#excluding target variable\n",
    "Calculate_vif(df[[i for i in df.describe().columns if i not in ['Rented_Bike_Count']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMrWgOZYr1Lt"
   },
   "outputs": [],
   "source": [
    "# As per above vif calculation dropping  dew_point_temperature columns.\n",
    "df.drop(['Dew_point_temperature','Day'],inplace=True,axis=1)\n",
    "Calculate_vif(df[[i for i in df.describe().columns if i not in ['Rented_Bike_Count']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrxJNetHSYQf"
   },
   "outputs": [],
   "source": [
    "# Creating dummy variables\n",
    "df=pd.get_dummies(df,drop_first=True,sparse=True)\n",
    "# checking modified data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkXgeFx22vhH"
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable to check for skewness\n",
    "fig = plt.figure()\n",
    "sns.distplot((df['Rented_Bike_Count']), bins = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elYEV38DGSe3"
   },
   "source": [
    "As the target variable is right skewed, we can normalize it by applying  either squareroot or log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbX0qcXCGR-J"
   },
   "outputs": [],
   "source": [
    "# log tranformation (adding 1 to bike count to avoid log(0) error)\n",
    "df['log(Rented_Bike_Count)'] = np.log10(df['Rented_Bike_Count']+1)\n",
    "fig = plt.figure()\n",
    "sns.distplot(df['log(Rented_Bike_Count)'], bins = 10)\n",
    "\n",
    "#square_root transformation\n",
    "df['sqrt(Rented_Bike_Count)'] = np.sqrt(df['Rented_Bike_Count'])\n",
    "fig = plt.figure()\n",
    "sns.distplot(df['sqrt(Rented_Bike_Count)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5w67JowJ5ZZq"
   },
   "outputs": [],
   "source": [
    "print(f\"Skewness of Target Variable : {df['Rented_Bike_Count'].skew():.2f}\")\n",
    "#print(f\"Skewness of Logarithm of Target Variable : {df['log(Rented_Bike_Count)'].skew():.2f}\")\n",
    "print(f\"Skewness of Square root of Target Variable : {df['sqrt(Rented_Bike_Count)'].skew():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVUgQtK16ytw"
   },
   "source": [
    "Here, we could see that the target variable is balanced better, after applying square root transformation and the skewness is greatly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kVn0Ezsb0Nh"
   },
   "outputs": [],
   "source": [
    "df.drop(['log(Rented_Bike_Count)','sqrt(Rented_Bike_Count)'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrGwM3B6T6oM"
   },
   "source": [
    "#### ***Defining Function to train the models :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wno5Cg2uQuvZ"
   },
   "outputs": [],
   "source": [
    "# creating a list so that all models evalution scores will be appended to the corrosponding list after hyperarameter\n",
    "mean_absolute_error_tuned = []\n",
    "mean_sq_error_tuned=[]\n",
    "root_mean_sq_error_tuned=[]\n",
    "Train_accuracy_score_tuned =[]\n",
    "Test_accuracy_score_tuned =[]\n",
    "r2_list_tuned=[]\n",
    "adjusted_r2_list_tuned=[]\n",
    "\n",
    "# creating a function for fit, predict and evaluting the models and append all evalution score in list \n",
    "def model_train(model,X_train,X_test,Y_train,Y_test, linear = False):\n",
    "  '''\n",
    "    train the model and gives mae, mse,rmse,r2 and adjusted r2 scores of the model \n",
    "  '''\n",
    "  #training the model\n",
    "  model.fit(X_train,Y_train)\n",
    "  # Accuracy Score\n",
    "  train_accuracy  = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 10).mean()\n",
    "  test_accuracy  = cross_val_score(estimator = model, X = X_test, y = y_test, cv = 10).mean()\n",
    "  print('*'*30, 'Cross validation Accuracy score', '*'*30)\n",
    "  print(\"Train Accuracy score  :\", train_accuracy)\n",
    "  print(\"Test Accuracy score  :\", test_accuracy)\n",
    "\n",
    "  print('\\n')\n",
    "\n",
    "  try:\n",
    "      # finding the best parameters of the model if any \n",
    "    print('*'*20, 'Best Parameters & Best Score', '*'*20)\n",
    "    print(f\"The best parameter that was found out is :{model.best_params_} \\nwhere model's best score is:  {model.best_score_} \\n\")\n",
    "  except:\n",
    "    print('None')\n",
    "    \n",
    "    \n",
    "\n",
    "  #predicting the Test set and evaluting the models \n",
    "  print('\\n')\n",
    "  print('*'*20, 'Evalution Metrics', '*'*20)\n",
    "\n",
    "  if linear == True:\n",
    "    Y_pred = model.predict(X_test)\n",
    "    #squaring the target variable test and prediction values as we took y as square root of target variable in modeling.\n",
    "    #finding mean_absolute_error\n",
    "    MAE  = mean_absolute_error(Y_test**2,Y_pred**2)\n",
    "    print(\"MAE :\" , MAE)\n",
    "\n",
    "    #finding mean_squared_error\n",
    "    MSE  = mean_squared_error(Y_test**2,Y_pred**2)\n",
    "    print(\"MSE :\" , MSE)\n",
    "\n",
    "    #finding root mean squared error\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    print(\"RMSE :\" ,RMSE)\n",
    "\n",
    "    #finding the r2 score\n",
    "    '''\n",
    "    R2 assumes that every single variable explains the variation in the dependent variable\n",
    "    '''\n",
    "    r2 = r2_score(Y_test**2,Y_pred**2)\n",
    "    print(\"R2 :\" ,r2)\n",
    "\n",
    "    #finding the adjusted r2 score\n",
    "    '''\n",
    "    The adjusted R2 gives the percentage of variation explained by \n",
    "    only the independent variables that actually affect the dependent variable.\n",
    "    '''\n",
    "    adj_r2=1-(1-r2_score(Y_test**2,Y_pred**2))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
    "    print(\"Adjusted R2 : \",adj_r2,'\\n')\n",
    "  \n",
    "  else:\n",
    "    # for tree base models\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    #finding mean_absolute_error\n",
    "    MAE  = mean_absolute_error(Y_test,Y_pred)\n",
    "    print(\"MAE :\" , MAE)\n",
    "\n",
    "    #finding mean_squared_error\n",
    "    MSE  = mean_squared_error(Y_test,Y_pred)\n",
    "    print(\"MSE :\" , MSE)\n",
    "\n",
    "    #finding root mean squared error\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    print(\"RMSE :\" ,RMSE)\n",
    "\n",
    "    #finding the r2 score\n",
    "    \n",
    "    r2 = r2_score(Y_test,Y_pred)\n",
    "    print(\"R2 :\" ,r2)\n",
    "    #finding the adjusted r2 score\n",
    "    adj_r2=1-(1-r2_score(Y_test,Y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
    "    print(\"Adjusted R2 : \",adj_r2,'\\n')\n",
    "\n",
    "    #Top 10 feature importance graph\n",
    "    try:\n",
    "      best = model.best_estimator_\n",
    "      features = new_X.columns\n",
    "      importances = best.feature_importances_[0:10]\n",
    "      indices = np.argsort(importances)\n",
    "      plt.figure(figsize=(10,15))\n",
    "      plt.title('Feature Importance')\n",
    "      plt.barh(range(len(indices)), importances[indices], color='blue',edgecolor='pink' ,align='center')\n",
    "      plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "      plt.xlabel('Relative Importance')\n",
    "      plt.show()\n",
    "    \n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "  # Here we appending the parameters for all models \n",
    "  mean_absolute_error_tuned.append(MAE)\n",
    "  mean_sq_error_tuned.append(MSE)\n",
    "  root_mean_sq_error_tuned.append(RMSE)\n",
    "  Train_accuracy_score_tuned.append(train_accuracy)\n",
    "  Test_accuracy_score_tuned.append(test_accuracy)\n",
    "  r2_list_tuned.append(r2)\n",
    "  adjusted_r2_list_tuned.append(adj_r2)\n",
    "\n",
    "  # print the coefficient and intercept for linear models\n",
    "  if model == LR:\n",
    "    print(\"*\"*25, \"coefficient\", \"*\"*25)\n",
    "    print(pd.DataFrame(model.coef_,X.columns, columns=['Coefficient']))\n",
    "    print('\\n')\n",
    "    print(\"*\"*25, \"Intercept\", \"*\"*25)\n",
    "    print('\\n')\n",
    "    print(model.intercept_)\n",
    "  else:\n",
    "    pass\n",
    "  print('\\n')\n",
    "  \n",
    "  print('*'*20, 'ploting the graph of actual and predicted only with 100 observation', '*'*20)\n",
    "\n",
    "  # ploting the graph of Actual and predicted only with 100 observation for better visualisation which model have these parameters and else we just pass them\n",
    "  try:\n",
    "    # ploting the line graph of actual and predicted values  \n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot((Y_pred)[:100])\n",
    "    plt.plot((np.array(Y_test)[:100]))\n",
    "    plt.legend([\"Predicted\",\"Actual\"])\n",
    "    plt.show()\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHmGjhC7TXMG"
   },
   "source": [
    "#### ***Linear models without hyperparameter tuning:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wDq5JJLTNbT"
   },
   "outputs": [],
   "source": [
    "mean_abs_error = []\n",
    "mean_sq_error=[]\n",
    "root_mean_sq_error=[]\n",
    "train_accuracy_score =[]\n",
    "test_accuracy_score =[]\n",
    "r2_list=[]\n",
    "adj_r2_list=[]\n",
    "\n",
    "# Define X-variable(Independent Variables) and Y-Variable(Dependent Variable)\n",
    "X=df.drop('Rented_Bike_Count',axis=1)\n",
    "y=np.sqrt(df['Rented_Bike_Count'])\n",
    "# Split data into train test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "#scaling the features\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "# Checking shape\n",
    "print(f'Shape of X_train : {X_train.shape}')\n",
    "print(f'Shape of y_train : {y_train.shape}')\n",
    "print(f'Shape of X_test : {X_test.shape}')\n",
    "print(f'Shape of y_test : {y_test.shape}')\n",
    "\n",
    "#instantiating the model\n",
    "lr = LinearRegression()\n",
    "l1 = Lasso()\n",
    "l2 = Ridge()\n",
    "e  = ElasticNet()\n",
    "\n",
    "linear_models = [lr,l1,l2,e]\n",
    "for i in linear_models:\n",
    "  print('\\n')\n",
    "  print('*'*20,i,'*'*20) \n",
    "  i.fit(X_train,y_train)\n",
    "  y_pred = i.predict(X_test)\n",
    "  mae = mean_absolute_error(y_test**2,y_pred**2)\n",
    "  print(\"MAE :\" , mae)\n",
    "  mse = mean_squared_error(y_test**2,y_pred**2)\n",
    "  print(\"MSE :\" , mse)\n",
    "  rmse = np.sqrt(mse)\n",
    "  print(\"RMSE :\" , rmse)\n",
    "  r2 = r2_score(y_test**2,y_pred**2)\n",
    "  print(\"R2 :\" , r2)\n",
    "  adj_r2 =1-(1-r2)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
    "  print(\"Adjusted_R2 :\" , adj_r2)\n",
    "  train_accuracy=i.score(X_train,y_train)\n",
    "  print(\"Cross validation accuracy:\" , train_accuracy)\n",
    "  test_accuracy=i.score(X_train,y_train)\n",
    "  print(\"Cross validation accuracy:\" , test_accuracy)\n",
    "\n",
    "\n",
    "  train_accuracy_score.append(train_accuracy)\n",
    "  test_accuracy_score.append(test_accuracy)\n",
    "  mean_abs_error.append(mae)\n",
    "  mean_sq_error.append(mse)\n",
    "  root_mean_sq_error.append(rmse)\n",
    "  r2_list.append(r2)\n",
    "  adj_r2_list.append(adj_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzbZXNpscVre"
   },
   "source": [
    "#### ***Linear Models with Hyperparameter tuning :***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1qYsXZmTBX0"
   },
   "source": [
    "######**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgLi3NZldZaa"
   },
   "outputs": [],
   "source": [
    "LR= LinearRegression()\n",
    "model_train(LR,X_train,X_test,y_train,y_test,linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG8cRDXxfV1u"
   },
   "source": [
    "######**Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQJ8RescfU43"
   },
   "outputs": [],
   "source": [
    "# Using Grid Search CV for Hyperparameter tuning of Lasso Regression model\n",
    "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014]} #lasso parameters \n",
    "L1 = GridSearchCV(Lasso(), parameters, cv=5) #using gridsearchcv and cross validation on the model\n",
    "# fit and evaluate model \n",
    "model_train(L1,X_train,X_test,y_train,y_test,linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzvAGighggsL"
   },
   "source": [
    "######**Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlVVUDkfgfvi"
   },
   "outputs": [],
   "source": [
    "# Using Grid Search CV for Hyperparameter tuning for Lasso Regression\n",
    "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100,0.5,1.5,1.6,1.7,1.8,1.9]} # giving parameters \n",
    "L2 = GridSearchCV(Ridge(), parameters, cv=5) #using gridsearchcv and cross validate the model\n",
    "# fit and evaluate model with score_matrix function\n",
    "model_train(L2,X_train,X_test,y_train,y_test,linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZokaY3DEg8r6"
   },
   "source": [
    "######**Elastic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1x1RzVVhHsv"
   },
   "outputs": [],
   "source": [
    "# Using Grid Search CV for Hyperparameter tuning for Lasso Regression\n",
    "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100,0.5,1.5,1.6,1.7,1.8,1.9]} # giving parameters \n",
    "E = GridSearchCV(ElasticNet(), parameters, cv=5) #using gridsearchcv and cross validate the model\n",
    "# fit and evaluate model with score_matrix function\n",
    "model_train(E,X_train,X_test,y_train,y_test,linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOLCdqQjhl3G"
   },
   "source": [
    "######**Polynomial Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diskcgIehmTR"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "P = PolynomialFeatures(degree=2,include_bias = False) #creating variables with degree 4\n",
    "P_X_train = P.fit_transform(X_train) # fit the train set\n",
    "P_X_test = P.transform(X_test) #transform the test set\n",
    "# fit and evaluate model with score_matrix function\n",
    "model_train(LR,X_train,X_test,y_train,y_test,linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Tq93pZrmorv"
   },
   "source": [
    "#### ***Data Splitting for Tree based Models:***\n",
    "As Desicion trees make no assumptions on relationships between features, so we can use all the independent features because multi collinearity won't affect the model.Also, we are not transforming the target variable here, as it's distribution won't have impact on model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wxOuDxnmmr0"
   },
   "outputs": [],
   "source": [
    "# Creating DataFrame for tree base models\n",
    "t_df = pd.get_dummies(Bikes_df,columns=['Seasons','Holiday','Functioning_Day','Month','Hour','Weekend'],drop_first=True)\n",
    "t_df.drop(['Day'],inplace=True,axis=1)\n",
    "# Split data in X and Y\n",
    "new_X = t_df.drop(columns='Rented_Bike_Count')\n",
    "new_y = t_df['Rented_Bike_Count']\n",
    "# Train test split our data\n",
    "X_train,X_test,y_train,y_test = train_test_split(new_X,new_y, test_size=0.25)\n",
    "#Feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train  = scaler.fit_transform(X_train) #fitting the X_train\n",
    "X_test   = scaler.transform(X_test) # transform test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JovWWQFVxF-N"
   },
   "source": [
    "##### ***Tree Based Models without Hyper Parameter tuning :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx09iMOqxgRr"
   },
   "outputs": [],
   "source": [
    "#instantiating the models\n",
    "dt= DecisionTreeRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "xgb = XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "tree_models = [dt,rf,xgb]\n",
    "for i in tree_models:\n",
    "  print('\\n')\n",
    "  print('*'*20,i,'*'*20) \n",
    "  i.fit(X_train,y_train)\n",
    "  y_pred = i.predict(X_test)\n",
    "  mae = mean_absolute_error(y_test**2,y_pred**2)\n",
    "  print(\"MAE :\" , mae)\n",
    "  mse = mean_squared_error(y_test**2,y_pred**2)\n",
    "  print(\"MSE :\" , mse)\n",
    "  rmse = np.sqrt(mse)\n",
    "  print(\"RMSE :\" , rmse)\n",
    "  r2 = r2_score(y_test**2,y_pred**2)\n",
    "  print(\"R2 :\" , r2)\n",
    "  adj_r2 =1-(1-r2)*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
    "  print(\"Adjusted_R2 :\" , adj_r2)\n",
    "  train_accuracy=i.score(X_train,y_train)\n",
    "  print(\"Train_Accuracy :\",train_accuracy)\n",
    "  test_accuracy=i.score(X_test,y_test)\n",
    "  print(\"Test_Accuracy :\",test_accuracy)\n",
    "  train_accuracy_score.append(train_accuracy)\n",
    "  test_accuracy_score.append(test_accuracy)\n",
    "  mean_abs_error.append(mae)\n",
    "  mean_sq_error.append(mse)\n",
    "  root_mean_sq_error.append(rmse)\n",
    "  r2_list.append(r2)\n",
    "  adj_r2_list.append(adj_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vTMZxzR6MB9"
   },
   "source": [
    "#### ***Tree based models with Hyper Parameter tuning :***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyPqD1Fk6awZ"
   },
   "source": [
    "##### ***Decision Tree Regression :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDAw2LGV6gqF"
   },
   "outputs": [],
   "source": [
    "# Parameters for Decission Tree model\n",
    "param_grid = {'criterion':['mae','rmse'],\n",
    "              'splitter':['best','random'],\n",
    "              'max_depth' : [5,10 ,15],\n",
    "              'max_features':['auto','log2']\n",
    "}\n",
    "# Using Grid Search \n",
    "DT= GridSearchCV(DecisionTreeRegressor(),param_grid=param_grid,cv=5)\n",
    "# fit and evaluate model \n",
    "model_train(DT,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1rYSma-TlW5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "plot_tree(dt, feature_names=new_X.columns, max_depth=3, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqKGLV8JRfca"
   },
   "source": [
    "##### ***Random Forest Regressor :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cuXeUJyHMwew"
   },
   "outputs": [],
   "source": [
    "param_grid={'n_estimators':[50,100, 150,200,],'max_depth':[3,5,10],'max_features':['auto','sqrt','log2']}\n",
    "# Using Grid SearchCV \n",
    "RF= GridSearchCV(RandomForestRegressor(),param_grid=param_grid,cv=5)\n",
    "# fit and evaluate model \n",
    "model_train(RF,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUVZZcf1WEMk"
   },
   "source": [
    "##### ***Extreme Gradient Booster Regressor*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5FMz2LGWLQn"
   },
   "outputs": [],
   "source": [
    "param_grid ={\n",
    "          'subsample': [0.9,0.7,0.5,0.3], \n",
    "          'n_estimators': [50,100,150],  \n",
    "          'max_depth': range(2,8,2), \n",
    "          'learning_rate': [0.02,0.04,0.06], \n",
    "          'eval_mertric': ['rmse','mse']   \n",
    "          }\n",
    "\n",
    "XGB= GridSearchCV(XGBRegressor(objective ='reg:squarederror'),param_grid=param_grid,cv=5)\n",
    "# fit and evaluate model \n",
    "model_train(XGB,X_train,X_test,y_train,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWpn7gveaYMY"
   },
   "source": [
    "#### ***Tree Based Model Explainability*** :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcEjy2onb8p9"
   },
   "source": [
    "##### ***LIME :***\n",
    "\n",
    "> LIME stands for Local Interpretable Model-agnostic Explanations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Mzeb2gVahmx"
   },
   "outputs": [],
   "source": [
    "def me_lime(model,num_features,row_number):\n",
    "  test = pd.DataFrame(X_test)\n",
    "  lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "      training_data = X_train,\n",
    "      feature_names = new_X.columns.values,\n",
    "      categorical_features=t_df.select_dtypes(include='category'),\n",
    "      mode = 'regression',\n",
    "      verbose=True,\n",
    "      random_state =42 )\n",
    "\n",
    "  lime_exp = lime_explainer.explain_instance(\n",
    "      data_row = new_X.iloc[row_number],\n",
    "      predict_fn = model.predict,num_features=num_features,\n",
    "  )\n",
    "  return lime_exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1xyEy0D-f5E"
   },
   "outputs": [],
   "source": [
    " #Calling lime function and taking only top 5 number of feature and 2nd Row\n",
    "me_lime(DT.best_estimator_,6,2)\n",
    "me_lime(RF.best_estimator_,6,2)\n",
    "me_lime(XGB.best_estimator_,6,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0fkL_Tx_Zdh"
   },
   "source": [
    "#### ***Creating Pickle File to save model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sl1a4O02_W31"
   },
   "outputs": [],
   "source": [
    "# Save the Model to file in the current working directory\n",
    "Pkl_Filename = \"Pickle_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(XGB, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAlaaB6NNiuf"
   },
   "source": [
    "#### ***Reusing Model from Pickel File :***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkNe1k8LNh1n"
   },
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_XGB_Model = pickle.load(file)\n",
    "Pickled_XGB_Model\n",
    "\n",
    "# Use the Reloaded Model to Calculate the accuracy score and predict target values\n",
    "# Calculate the Score \n",
    "score = Pickled_XGB_Model.score(X_test, y_test)  \n",
    "# Print the Score\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))  \n",
    "\n",
    "# Predict the Labels using the reloaded Model\n",
    "Ypredict = Pickled_XGB_Model.predict(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JO2Georep8p"
   },
   "source": [
    "#### ***Evaluation metrics Comparision :***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDaaqC7ve-yk"
   },
   "source": [
    "#####  ***Without Hyper parameter tuning:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW05a5hveqcQ"
   },
   "outputs": [],
   "source": [
    "# creating a Data frame of model scores without hyperparameter tuning\n",
    "model_names = ['Linear Regression','Ridge','Lasso','Elastic','Decision_Tree','Random_Forest','xg_boost']\n",
    "#creating dictionary to store all the metrices \n",
    "all_metrics={'Mean_Absolute_error':mean_abs_error,'Mean_square_error':mean_sq_error,'Root_Mean_square_error':root_mean_sq_error,'Train_Accuracy_score':train_accuracy_score,'Test_Accuracy_score':test_accuracy_score,'R2':r2_list,'Adjusted_R2':adj_r2_list}\n",
    "#converting dictionary to dataframe for simple visualization \n",
    "metrics=pd.DataFrame.from_dict(all_metrics,orient='index',columns=model_names)\n",
    "# Adjusted R2 score is set in descending order \n",
    "metrics = metrics.transpose().sort_values(\"Adjusted_R2\",ascending=False).reset_index().rename(columns={'index':'model'})\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7VqlADEfjg4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(y='Adjusted_R2', x = 'model', data=metrics)\n",
    "plt.title(\"Adjusted R2 with respect to Models\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcUPhv0Cf79A"
   },
   "source": [
    "#####  ***With Hyper parameter tuning:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i90Uksdlf-Wp"
   },
   "outputs": [],
   "source": [
    "# creating a Data frame of model scores without hyperparameter tuning\n",
    "model_names = ['Linear Regression','Ridge','Lasso','Elastic','Polynomial','Decision_Tree','Random_Forest','xg_boost']\n",
    "#creating dictionary to store all the metrices \n",
    "all_metrics_tuned={'Mean_Absolute_error':mean_absolute_error_tuned,'Mean_square_error':mean_sq_error_tuned,'Root_Mean_square_error':root_mean_sq_error_tuned,'Train_Accuracy_score':Train_accuracy_score_tuned,'Test_Accuracy_score':Test_accuracy_score_tuned,'R2':r2_list_tuned,'Adjusted_R2':adjusted_r2_list_tuned}\n",
    "#converting dictionary to dataframe for simple visualization \n",
    "metrics_tuned=pd.DataFrame.from_dict(all_metrics_tuned,orient='index',columns=model_names)\n",
    "# Adjusted R2 score is set in descending order \n",
    "metrics_tuned = metrics_tuned.transpose().sort_values(\"Adjusted_R2\",ascending=False).reset_index().rename(columns={'index':'model'})\n",
    "metrics_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xu6f3ntYhGIA"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(y='Adjusted_R2', x = 'model', data=metrics_tuned)\n",
    "plt.title(\"Adjusted R2 with respect to Tuned Models\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LezHokZ7-O64"
   },
   "source": [
    "### **Reinforcement Learnning Approach :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUwtA__O-89O"
   },
   "source": [
    "##### ***Environment*** :\n",
    "    The environment module is a bike station object that does the following functions:\n",
    "    1.generates simulated and real bike stock data\n",
    "    2.delivers feedback to the RL agent in the form of bike inventory, incentives/penalties,episode termination status \n",
    "    3.updates the bike stock in response to the action taken by the RL agent\n",
    "    4.resets the environment for a new training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCR7Yhxe_JDl"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class bikestation():\n",
    "    \n",
    "    def __init__(self, mode, debug):    \n",
    "        print(\"Creating A Bike Station Environment!!!\")\n",
    "        self.mode = mode\n",
    "        self.seed = np.random.random_integers(0, 10)\n",
    "        self.num_hours = 23\n",
    "        self.current_hour = 0\n",
    "        self.bike_count_sim = self.generate_count(mode)\n",
    "         # to be reset to original environment after  every episode\n",
    "        self.bike_count = self.bike_count_sim.copy()\n",
    "        self.old_count = self.bike_count[0]\n",
    "        self.new_count = 0\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.bike_moved = 0\n",
    "        self.debug = debug\n",
    "        self.actions = [-15, -10, -5, 0]\n",
    "        self.n_actions = len(self.actions)\n",
    "        #features of the observation: hour, old count, new count\n",
    "        self.n_features = 1\n",
    "        self.game_over = False\n",
    "        if self.debug == True:\n",
    "            print(\"Generating Bike count: {}\".format(self.mode))\n",
    "            print(\"Bike count: {}\".format(self.bike_count))\n",
    "        \n",
    "    def generate_count(self, mode):\n",
    "        # generate a list of 24 hourly bike count based on mode\n",
    "        # mode: linear which is addition of 3 bikes continuously , random is addition of 3 bikes with a random integer between -5 to 5.\n",
    "        #initialising the bike stock to be 30 \n",
    "        bike_count = [20]\n",
    "        if mode == \"linear\":\n",
    "            for i in range(1, 24):\n",
    "                bike_count.append(bike_count[i-1]+3)\n",
    "        if mode == \"random\":\n",
    "            for i in range(1, 24):\n",
    "                bike_count.append(bike_count[i-1] + 3 + np.random.random_integers(-5, 5))\n",
    "        return bike_count\n",
    "\n",
    "    def feedback(self, action):\n",
    "        # share back count, reward ,and termination status to agent\n",
    "        if self.debug == True:\n",
    "            print(\"Current Hour: {}\".format(self.current_hour))\n",
    "            print(\"Current count: {}\".format(self.bike_count[self.current_hour]))\n",
    "            print(\"Bikes Moved in Last Hour: {}\".format(self.bike_moved))\n",
    "            print(\"Collect {} rewards\".format(self.reward))\n",
    "            print(\"Will move {} bikes\".format(action))\n",
    "            print(\"---\")\n",
    "        if action != 0:\n",
    "            #if there is any rebalancing happening \n",
    "            self.update_count(action)\n",
    "            self.reward = -0.5*np.abs(action)\n",
    "            #if the count is greater than 50 at any hour\n",
    "        if self.bike_count[self.current_hour] > 50:\n",
    "            self.reward = -30\n",
    "            #if count is less than 50 at any moment\n",
    "        if self.bike_count[self.current_hour] < 0:\n",
    "            self.reward = -30\n",
    "        if self.current_hour == 23:\n",
    "            #If count is between 0 to 50  at last hour\n",
    "            if (self.bike_count[self.current_hour] <= 50)&(self.bike_count[self.current_hour] > 0):\n",
    "                self.reward = 20\n",
    "            else: \n",
    "                self.reward = -20\n",
    "            self.done = True\n",
    "            #self.new_count = 'terminal'\n",
    "            self.game_over = True\n",
    "        # update to next hour\n",
    "        if self.current_hour != 23:\n",
    "            self.update_hour()\n",
    "            self.old_count = self.bike_count[self.current_hour - 1]\n",
    "            self.new_count = self.bike_count[self.current_hour]\n",
    "        return self.current_hour, self.old_count, self.new_count, self.reward, self.done, self.game_over\n",
    "    \n",
    "    def get_old_count(self):\n",
    "        return self.old_count\n",
    "    \n",
    "    def update_count(self, num_bike):\n",
    "        # update bike count based on RL Agent action \n",
    "        if self.current_hour != 23:\n",
    "            for hour in range(self.current_hour+1, len(self.bike_count)):\n",
    "                self.bike_count[hour] += num_bike\n",
    "            self.bike_moved = num_bike\n",
    "        else:\n",
    "            if self.debug == True:\n",
    "                print(\"Last Hour!!! Can't Move Bikes Further!!!\")\n",
    "            pass\n",
    "        return\n",
    "    \n",
    "    def update_hour(self):\n",
    "        # update current_hour \n",
    "        self.current_hour += 1\n",
    "        if self.debug == True:\n",
    "            print(\"Forwarding to Current Hour\")\n",
    "        return\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.debug == True:\n",
    "            print(\"Resetting The Environment ...\")\n",
    "        self.num_hours = 23\n",
    "        self.current_hour = 0\n",
    "        self.bike_count = self.bike_count_sim.copy()\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.bike_moved = 0\n",
    "        self.old_count = self.bike_count[0]\n",
    "        self.new_count = 0\n",
    "        #return (self.current_hour, self.old_count, self.new_count)\n",
    "        \n",
    "    def current_count(self):\n",
    "        return self.bike_count[self.current_hour]\n",
    "    \n",
    "    def get_sim_count(self):\n",
    "        return self.bike_count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R86ANTmDCEx4"
   },
   "source": [
    "##### ***`Agent :`***\n",
    "\n",
    ">  Agent is analagous to Bike station Operator.The agent's action is the number of bikes she can move per hour. At any given hour, the agent has the option of moving 0, 5, 10, or 15 bikes. The reward and punishment structure are as follows:\n",
    "\n",
    "*   -30 if the hourly bike stock falls outside the range [0, 50].\n",
    "*   +20 if bike stock is in the range [0, 50] at 23 hours; otherwise, -20\n",
    "*   -0.5  times the number of bikes eliminated every hour.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecdgjzk3CI5o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class agent():\n",
    "\n",
    "    def __init__(self, epsilon, lr, gamma, current_count, debug):\n",
    "        print(\"Creating an Agent !!!\")\n",
    "        #Defining actions which is count of bikes that can be moved at a certain hour\n",
    "        self.actions = [-15, -10, -5, 0]\n",
    "        self.reward = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.debug = debug\n",
    "        self.current_count = current_count\n",
    "        # performance metric\n",
    "        self.q_table = pd.DataFrame(columns = self.actions, dtype = np.float64)\n",
    "        self.hourly_action_history = []\n",
    "        self.hourly_count_history = []\n",
    "       \n",
    "    def choose_action(self, s):\n",
    "        '''\n",
    "        This funciton choose an action based on Q Table. It also does \n",
    "        validation to ensure count will not be negative after moving bikes.\n",
    "        Input: \n",
    "            - s: current bike count\n",
    "        \n",
    "        Output:\n",
    "            - action: number of bikes to move\n",
    "        '''\n",
    "        self.check_state_exist(s)\n",
    "        self.current_count = s\n",
    "        # find valid action based on current count \n",
    "        valid_state_action = self.find_valid_action(self.q_table.loc[s, :])\n",
    "        valid_state_action = self.q_table.loc[s, :]\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            try:\n",
    "                # find the action with the highest expected reward\n",
    "                valid_state_action = valid_state_action.reindex(np.random.permutation(valid_state_action.index))\n",
    "                action = valid_state_action.idxmax()\n",
    "            except:\n",
    "                # if action list is null, default to 0\n",
    "                action = 0\n",
    "            if self.debug == True:\n",
    "                print(\"Decided to Move: {}\".format(action))\n",
    "        else:\n",
    "            # randomly choose an action\n",
    "            try:\n",
    "                action = np.random.choice(valid_state_action.index)\n",
    "            except:\n",
    "                action = 0\n",
    "            if self.debug == True:\n",
    "                print(\"Randomly Move: {}\".format(action))\n",
    "        self.hourly_action_history.append(action)\n",
    "        self.hourly_count_history.append(s)\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_, g):\n",
    "        '''\n",
    "        This function updates Q tables after each interaction with the\n",
    "        environment.\n",
    "        Input: \n",
    "            - s: current bike count\n",
    "            - a: current action (number of bikes to move)\n",
    "            - r: reward received from current state\n",
    "            - s_: new bike count based on bike moved and new count\n",
    "        Output: None\n",
    "        '''\n",
    "        if self.debug == True:\n",
    "            print(\"Moved Bikes: {}\".format(a))\n",
    "            print(\"Old Bike count: {}\".format(s))\n",
    "            print(\"New Bike count: {}\".format(s_))\n",
    "            print(\"---\")\n",
    "        self.check_state_exist(s_)\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        if g == False:\n",
    "            # Updated Q Target Value if it is not end of day  \n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "        else:\n",
    "            # Update Q Target Value as Immediate reward if end of day\n",
    "            q_target = r\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "        return\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        # Add a new row with state value as index if not exist\n",
    "        if state not in self.q_table.index:\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                        [0]*len(self.actions), \n",
    "                        index = self.q_table.columns,\n",
    "                        name = state\n",
    "                        )\n",
    "                )\n",
    "        return\n",
    "    \n",
    "    def find_valid_action(self, state_action):\n",
    "        '''\n",
    "        This function check the validity acitons in a given state.\n",
    "        Input: \n",
    "            - state_action: the current state under consideration\n",
    "        Output:\n",
    "            - state_action: a pandas Series with only the valid actions that\n",
    "                            will not cause negative count\n",
    "        '''\n",
    "        # remove action that will count to be negative\n",
    "        for action in self.actions:\n",
    "            if self.current_count + action < 0:\n",
    "                if self.debug == True:\n",
    "                    print(\"Drop action {}, current count {}\".format(action, self.current_count))\n",
    "                state_action.drop(index = action, inplace = True)\n",
    "        return state_action\n",
    "        \n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(self.q_table)\n",
    "\n",
    "    def get_q_table(self):\n",
    "        return self.q_table\n",
    "\n",
    "    def get_hourly_actions(self):\n",
    "        return self.hourly_action_history\n",
    "    \n",
    "    def get_hourly_counts(self):\n",
    "        return self.hourly_count_history\n",
    "\n",
    "    def reset_hourly_history(self):\n",
    "        self.hourly_action_history = []\n",
    "        self.hourly_count_history = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJ6KJvl_Ebqt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "class trainer():\n",
    "    def __init__(self):\n",
    "        # Session Properties\n",
    "        self.episodes = []\n",
    "        self.count_type = \"\"\n",
    "        self.logging = False\n",
    "        self.env_debug = False\n",
    "        self.rl_debug = False\n",
    "        self.bike_station = None\n",
    "        self.operator = None\n",
    "        self.sim_count = []\n",
    "        # Performance Metric\n",
    "        self.success_ratio = 0\n",
    "        self.rewards = []  # [[r from session 1], [r from session 2] ...]\n",
    "        self.avg_rewards = [] #[np.mean([r from session 1]), np.mean([r from session 2])...]\n",
    "        self.final_counts = [] # [[count from session 1], [count from session 2] ...]\n",
    "        self.episode_action_history = []\n",
    "        self.episode_count_history = []\n",
    "        self.session_action_history = []\n",
    "        self.session_count_history = []\n",
    "        self.q_tables = []\n",
    "        self.actions = [-15, -10, -5, 0]\n",
    "        \n",
    "    def start(self, episodes, count_type, logging, env_debug, rl_debug):\n",
    "        self.episodes = episodes\n",
    "        self.count_type = count_type\n",
    "        self.logging = logging\n",
    "        self.env_debug = env_debug\n",
    "        self.rl_debug = rl_debug\n",
    "        idx = 0\n",
    "        for eps in self.episodes:\n",
    "            # Initiate new evironment and RL agent\n",
    "            self.bike_station = bikestation(self.count_type, debug = self.env_debug)\n",
    "            self.sim_count.append(self.bike_station.get_sim_count())\n",
    "\n",
    "            self.operator = agent(epsilon = 0.9, lr = 0.01, gamma = 0.9, current_count = self.bike_station.current_count(), debug = self.rl_debug)\n",
    "            # Train the RL agent and collect performance stats\n",
    "            rewards, final_counts = self.train_operator(idx, len(self.episodes), eps, logging = self.logging)\n",
    "            # Log the results from this training session\n",
    "            self.rewards.append(rewards)\n",
    "            self.avg_rewards.append(np.mean(rewards))\n",
    "            self.final_counts.append(final_counts)\n",
    "            #self.q_tables.append(self.operator.get_q_table())\n",
    "            self.session_action_history.append(self.episode_action_history)\n",
    "            self.session_count_history.append(self.episode_count_history)\n",
    "            self.reset_episode_history()\n",
    "            # Destroy the environment and agent objects\n",
    "            self.bike_station = None\n",
    "            self.operator = None\n",
    "            idx += 1\n",
    "        if logging == True:\n",
    "            self.save_session_results(self.get_timestamp(replace = True))\n",
    "        return\n",
    "    \n",
    "    def train_operator(self, idx, num_sessions, episodes, logging):\n",
    "        '''\n",
    "        This function trains an RL agent by interacting with the bike station \n",
    "        environment. It also tracks and reports performance stats.\n",
    "        Input:\n",
    "            - episodes: a int of episode to be trained in this session (e.g. 500)\n",
    "        Output:\n",
    "            - reward_list: a list of reward per episode in this sesison\n",
    "            - final_counts: a list of final counts per episode in this session\n",
    "        '''\n",
    "        print(\"Start training the Agent !!!\")\n",
    "        rewards = 0\n",
    "        reward_list = []\n",
    "        final_counts = []\n",
    "        step = 0\n",
    "        for eps in range(episodes):\n",
    "            self.bike_station.reset()\n",
    "            while True:\n",
    "                # Agent picks an action (number of bikes to move)\n",
    "                # Agent sends the action to bike station environment\n",
    "                # Agent gets feedback from the environment (e.g. reward of the action, new bike count after the action, etc.)\n",
    "                # Agent \"learn\" the feedback by updating its Q-Table (state, action, reward)\n",
    "                # Repeat until end of day (23 hours)\n",
    "                # Reset bike station environment to start a new day, repeat all\n",
    "                action = self.operator.choose_action(self.bike_station.get_old_count())\n",
    "                current_hour, old_count, new_count, reward, done, game_over = self.bike_station.feedback(action)\n",
    "                #observation_, reward, done = self.bike_station.feedback(action)\n",
    "                if done == True:\n",
    "                    print(\"{} of {} Session | Episode: {} | Final count: {} |Final Reward: {:.2f}\".format(idx, \n",
    "                          num_sessions, eps, old_count, rewards))\n",
    "                    reward_list.append(rewards)\n",
    "                    final_counts.append(old_count)\n",
    "                    rewards = 0\n",
    "                    # Log session action history by episode\n",
    "                    self.episode_action_history.append(self.operator.get_hourly_actions())\n",
    "                    self.episode_count_history.append(self.operator.get_hourly_counts())\n",
    "                    self.operator.reset_hourly_history()               \n",
    "                    break\n",
    "                self.operator.learn(old_count, action, reward, new_count, game_over)\n",
    "                step +=1\n",
    "                rewards += reward                    \n",
    "        return reward_list, final_counts\n",
    "    \n",
    "    def get_timestamp(self, replace):\n",
    "        if replace == True:\n",
    "            return str(datetime.datetime.now()).replace(\" \", \"\").replace(\":\", \"\").\\\n",
    "                        replace(\".\", \"\").replace(\"-\", \"\")\n",
    "        else:\n",
    "            return str(datetime.datetime.now())\n",
    "    \n",
    "    def reset_episode_history(self):\n",
    "        self.episode_action_history = []\n",
    "        self.episode_count_history = []\n",
    "        \n",
    "    def cal_performance(self):\n",
    "        successful_counting = []\n",
    "        print(\"===== Performance =====\")\n",
    "        for session in range(len(self.final_counts)):\n",
    "            length = len(self.final_counts[session])\n",
    "            num_overcount = np.count_nonzero(np.array(self.final_counts[session]) > 50)\n",
    "            num_undercount = np.count_nonzero(np.array(self.final_counts[session]) <= 0)\n",
    "            ratio = (length - num_undercount - num_overcount)*100 / length\n",
    "            print(\"Session {} | Overcount {} Times | Undercount {} Times | {}% Successful\".format(session, num_overcount, \n",
    "                  num_undercount, ratio))\n",
    "            average_reward = round(self.avg_rewards[session], 2)\n",
    "            print(\"Average Episode Reward for Session: {}\".format(average_reward))\n",
    "            successful_counting.append(ratio)\n",
    "        return successful_counting\n",
    "    \n",
    "    def save_session_results(self, timestamp):\n",
    "        '''\n",
    "        This function logs the following: \n",
    "            - overall success ratio of each session\n",
    "            - Q Table of each session\n",
    "        '''\n",
    "        # --- create a session folder ---\n",
    "        dir_path = \"./performance_log/\" + timestamp\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "        successful_counting = self.cal_performance()\n",
    "        # --- Write Success Rate to File ---\n",
    "        fname = dir_path + \"/success_rate - \" + timestamp + \".txt\"\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write(\"Logged at {}\".format(self.get_timestamp(replace = False)))\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"This training session ran episodes: {}\".format(self.episodes))\n",
    "            f.write(\"\\n\")\n",
    "            for session in range(len(successful_counting)):\n",
    "                f.write(\"Session {} | Episodes: {} | Success Rate: {:.2f}%\".format(session, \n",
    "                        self.episodes[session], successful_counting[session]))\n",
    "                f.write(\"\\n\")\n",
    "        # --- Save Q tables --- \n",
    "        for session in range(len(self.q_tables)):\n",
    "            self.q_tables[session].to_csv(dir_path + \"/q_table_session_\" + \\\n",
    "                        str(session) + timestamp + \".csv\")\n",
    "\n",
    "\n",
    "episode_list = [eps for eps in range(2000, 8000, 2000)]\n",
    "data = input(\"Linear or Random?: \").lower()\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = trainer()\n",
    "    trainer.start(episode_list, data, logging = True, env_debug = False, rl_debug = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
